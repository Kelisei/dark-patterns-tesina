{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36841d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0555105",
   "metadata": {},
   "outputs": [],
   "source": [
    "URGENCY_TRIGGERS = [\n",
    "    \"apurate\", \"apúrate\", \"ya\", \"no te lo pierdas\", \"última\", \"ultima\", \"oportunidad\",\n",
    "    \"comprá\", \"compra\", \"reservá\", \"reserva\", \"oferta\", \"últimas\", \"ultimas\", \"flash\", \"sale\",\n",
    "    \"relámpago\", \"relampago\", \"aprovecha\", \"ahora o nunca\", \"por tiempo limitado\", \"ultimo día\",\n",
    "    \"último día\", \"última oportunidad\", \"ultima oportunidad\", \"solo hoy\", \"sólo hoy\",\n",
    "    \"solo ahora\", \"sólo ahora\", \"termina en\", \"finaliza en\", \"quedan\", \"queda\",\n",
    "    \"últimos\", \"ultimos\", \"stock bajo\", \"casi agotado\"\n",
    "]\n",
    "\n",
    "TECH_NOUNS = [\n",
    "    \"sesión\", \"sesion\", \"batería\", \"bateria\", \"dispositivo\", \"equipo\", \"sistema\",\n",
    "    \"conexión\", \"conexion\", \"proceso\", \"operación\", \"operacion\", \"pantalla\",\n",
    "    \"aplicación\", \"aplicacion\", \"instancia\", \"entorno\", \"pedido\", \"token\"\n",
    "]\n",
    "\n",
    "END_VERBS = [\n",
    "    \"expira\", \"expirar\", \"caduca\", \"caducar\", \"vence\", \"vencer\", \"cierra\", \"cerrar\",\n",
    "    \"finaliza\", \"finalizar\", \"termina\", \"terminar\", \"se agotará\", \"se agotara\",\n",
    "    \"agotarse\", \"apaga\", \"apagarse\", \"desconecta\", \"desconectarse\", \"bloquea\", \"bloquearse\"\n",
    "]\n",
    "\n",
    "EVENT_TERMS = [\"clase\", \"evento\", \"live\", \"streaming\", \"stream\", \"partido\", \"examen\"]\n",
    "EVENT_START_VERBS = [\"empieza\", \"comienza\", \"inicia\", \"arranca\", \"comenzar\", \"empezar\", \"iniciar\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5633fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MATCHER\n",
    "# ============================================================================\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "\n",
    "# REGEX SOLO PARA TIEMPO Y RELOJ (difícil de hacer con matchers)\n",
    "\n",
    "re_in_time = re.compile(r\"\\b\\d+\\s*(?:segundos?|minutos?|horas?|hs|h|m|s|d[ií]as?|d[ií]a)\\b\", re.IGNORECASE)\n",
    "re_clock = re.compile(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\")\n",
    "\n",
    "\n",
    "# INICIALIZAR MATCHERS\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "shaming_matcher = Matcher(nlp.vocab)  # Matcher específico para shaming\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "# MATCHERS PARA SHAMING (tienen prioridad - NO descartar)\n",
    "\n",
    "# Primera persona - verbos conjugados en primera persona singular\n",
    "shaming_matcher.add(\"FP_VERB\", [[{\"POS\": \"VERB\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}]])\n",
    "shaming_matcher.add(\"FP_COPULA\", [[{\"DEP\": \"cop\", \"POS\": \"AUX\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}]])\n",
    "shaming_matcher.add(\"FP_ME_VERB\", [[{\"POS\": \"PRON\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}, {\"POS\": \"VERB\"}]])\n",
    "\n",
    "# Perífrasis (voy a + verbo)\n",
    "shaming_matcher.add(\"FP_PERIFRASIS_VOY_A\", [[\n",
    "    {\"DEP\": \"aux\", \"POS\": \"AUX\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}},\n",
    "    {\"DEP\": \"mark\", \"POS\": \"ADP\"},\n",
    "    {\"POS\": \"VERB\"},\n",
    "]])\n",
    "\n",
    "# Patrones \"es lo mío\"\n",
    "shaming_matcher.add(\"FP_ES_LO_MIO\", [[\n",
    "    {\"LEMMA\": {\"IN\": [\"seguir\", \"ignorar\", \"ser\", \"hacer\"]}, \"POS\": {\"IN\": [\"VERB\", \"AUX\"]}},\n",
    "    {\"OP\": \"+\", \"POS\": {\"NOT_IN\": [\"PUNCT\"]}},\n",
    "    {\"LEMMA\": \"ser\", \"POS\": {\"IN\": [\"AUX\", \"VERB\"]}},\n",
    "    {\"LOWER\": \"lo\"},\n",
    "    {\"LOWER\": {\"IN\": [\"mío\", \"mio\"]}},\n",
    "]])\n",
    "\n",
    "# Ironía\n",
    "shaming_matcher.add(\"IRONIA_PREFIERO_NO\", [[{\"LEMMA\": \"preferir\", \"POS\": \"VERB\"}, {\"LOWER\": \"no\"}, {\"POS\": \"VERB\"}]])\n",
    "shaming_matcher.add(\"IRONIA_QUIEN_NECESITA\", [[{\"LOWER\": {\"IN\": [\"quién\", \"quien\"]}}, {\"LEMMA\": \"necesitar\", \"POS\": \"VERB\"}]])\n",
    "shaming_matcher.add(\"IRONIA_PORQUE_HABRIA_DE\", [[\n",
    "    {\"LOWER\": \"por\"}, {\"LOWER\": {\"IN\": [\"qué\", \"que\"]}}, {\"LEMMA\": \"haber\", \"POS\": \"AUX\"}, {\"LOWER\": \"de\"}, {\"POS\": \"VERB\"}\n",
    "]])\n",
    "\n",
    "# Metáforas con \"es mi\"\n",
    "shaming_matcher.add(\"META_VERBOS_ES_MI\", [[\n",
    "    {\"LEMMA\": {\"IN\": [\"ignorar\", \"vivir\", \"ser\", \"estar\", \"perder\", \"arruinar\", \"hacer\", \"rechazar\", \"fracasar\", \"seguir\"]}},\n",
    "    {\"OP\": \"*\"},\n",
    "    {\"LOWER\": \"es\"},\n",
    "    {\"LOWER\": \"mi\"},\n",
    "    {\"OP\": \"+\"}\n",
    "]])\n",
    "\n",
    "# ============================================================================\n",
    "# MATCHERS PARA METADATA (debe ser TODO el texto)\n",
    "# ============================================================================\n",
    "\n",
    "# Patrón: solo \"N colores/tamaños/talles/piezas/unidades\"\n",
    "matcher.add(\"METADATA_UNITS_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LIKE_NUM\": True},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"colores\", \"color\", \"tamaños\", \"tamaño\", \"talles\", \"talle\", \"piezas\", \"pieza\", \"unidades\", \"unidad\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Patrón: solo \"N ventas\"\n",
    "matcher.add(\"METADATA_SALES_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LIKE_NUM\": True},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"ventas\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Patrón: \"en N carritos\" o \"N carritos\"\n",
    "matcher.add(\"METADATA_CARTS_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"en\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LIKE_NUM\": True},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"carritos\", \"carrito\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LIKE_NUM\": True},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"carritos\", \"carrito\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Patrón: \"(N disponibles)\" o \"Stock disponible: (N disponibles)\"\n",
    "matcher.add(\"METADATA_AVAILABLE_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": \"(\"},\n",
    "        {\"LIKE_NUM\": True},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"disponibles\", \"disponible\", \"restantes\", \"restante\"]}},\n",
    "        {\"TEXT\": \")\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"stock\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"disponible\"},\n",
    "        {\"TEXT\": \":\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"TEXT\": \"(\"},\n",
    "        {\"LIKE_NUM\": True},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"disponibles\", \"disponible\"]}},\n",
    "        {\"TEXT\": \")\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Patrón: \"disponible en\" (solo eso)\n",
    "matcher.add(\"METADATA_AVAILABLE_IN\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"disponible\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"en\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# MATCHERS PARA CTAs NEUTRALES (debe ser TODO el texto)\n",
    "# ============================================================================\n",
    "\n",
    "# \"no gracias\" (solo eso)\n",
    "matcher.add(\"NEUTRAL_NO_THANKS_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"no\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"gracias\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"quiero saber más\", \"quiero más info\", etc. - REMOVIDO \"quiero\" genérico para evitar conflicto con shaming\n",
    "matcher.add(\"NEUTRAL_WANT_INFO_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"quiero\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"saber\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"más\", \"mas\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"quiero\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"más\", \"mas\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"info\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"quiero\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"aprender\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"seo\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"quiero\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"mi\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"código\", \"codigo\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"quiero\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"una\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"consultoría\", \"consultoria\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"digital\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"descubrir ahora\", \"descubrirlo ahora\"\n",
    "matcher.add(\"NEUTRAL_DISCOVER_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"descubrir\", \"descubrirlo\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"ahora\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"descargar ahora\"\n",
    "matcher.add(\"NEUTRAL_DOWNLOAD_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"descargar\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"ahora\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"cancelar mi suscripción\" - REMOVIDO para evitar conflicto con primera persona\n",
    "# (Si es shaming, será detectado por FP_VERB o FP_ME_VERB)\n",
    "\n",
    "# \"sí, por favor\", \"si lo quiero\", etc. - REMOVIDOS los que usan primera persona\n",
    "matcher.add(\"NEUTRAL_YES_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"si\", \"sí\"]}},\n",
    "        {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"por\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"favor\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"recibí/suscribite/anotate + alerta/aviso/lanzamiento\" - REMOVIDO \"recibí\" (primera persona)\n",
    "matcher.add(\"NEUTRAL_ALERTS_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"suscribite\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"al\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"aviso\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"de\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"lanzamiento\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"anotate\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"para\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"recibir\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"una\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"alerta\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"cuando\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"lancemos\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"mostrame N recetas...\"\n",
    "matcher.add(\"NEUTRAL_SHOW_RECIPES\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"mostrame\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LIKE_NUM\": True},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"recetas\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"participa ya\"\n",
    "matcher.add(\"NEUTRAL_PARTICIPATE\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"participa\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"ya\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# MATCHERS PARA LANZAMIENTOS (debe ser TODO el texto o empezar así)\n",
    "# ============================================================================\n",
    "\n",
    "# \"muy pronto\"\n",
    "matcher.add(\"LAUNCH_SOON_FULL\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"muy\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"pronto\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"disponible próximamente\"\n",
    "matcher.add(\"LAUNCH_AVAILABLE_SOON\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"disponible\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"próximamente\", \"proximamente\"]}},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"lanzamiento...\" o \"drop...\" (inicio del texto)\n",
    "matcher.add(\"LAUNCH_TERMS_START\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": {\"IN\": [\"lanzamiento\", \"drop\"]}}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"cuenta regresiva...\" o \"tiempo restante...\" (inicio)\n",
    "matcher.add(\"LAUNCH_COUNTDOWN_START\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"cuenta\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"regresiva\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"tiempo\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"restante\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"tu\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"contador\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"regresivo\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# \"en breve...\" (inicio)\n",
    "matcher.add(\"LAUNCH_BRIEF_START\", [\n",
    "    [\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"en\"},\n",
    "        {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "        {\"LOWER\": \"breve\"}\n",
    "    ]\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# PHRASE MATCHERS PARA DETECCIÓN DE TÉRMINOS (no todo el texto)\n",
    "# ============================================================================\n",
    "\n",
    "# Triggers de urgencia\n",
    "urgency_patterns = [nlp.make_doc(trigger) for trigger in URGENCY_TRIGGERS]\n",
    "phrase_matcher.add(\"URGENCY_TRIGGERS\", urgency_patterns)\n",
    "\n",
    "# Términos técnicos\n",
    "tech_noun_patterns = [nlp.make_doc(noun) for noun in TECH_NOUNS]\n",
    "phrase_matcher.add(\"TECH_NOUNS\", tech_noun_patterns)\n",
    "\n",
    "# Verbos de finalización\n",
    "end_verb_patterns = [nlp.make_doc(verb) for verb in END_VERBS]\n",
    "phrase_matcher.add(\"END_VERBS\", end_verb_patterns)\n",
    "\n",
    "# Términos de eventos\n",
    "event_patterns = [nlp.make_doc(term) for term in EVENT_TERMS]\n",
    "phrase_matcher.add(\"EVENT_TERMS\", event_patterns)\n",
    "\n",
    "# Verbos de inicio de eventos\n",
    "event_start_patterns = [nlp.make_doc(verb) for verb in EVENT_START_VERBS]\n",
    "phrase_matcher.add(\"EVENT_START\", event_start_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8edb94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_shaming_pattern(doc):\n",
    "    \"\"\"Detecta si hay patrones de shaming - TIENEN PRIORIDAD\"\"\"\n",
    "    matches = shaming_matcher(doc)\n",
    "    return len(matches) > 0\n",
    "\n",
    "def has_urgency_trigger(doc):\n",
    "    \"\"\"Detecta si hay triggers de urgencia en CUALQUIER parte del texto\"\"\"\n",
    "    matches = phrase_matcher(doc, as_spans=False)\n",
    "    return any(nlp.vocab.strings[match_id] == \"URGENCY_TRIGGERS\" for match_id, _, _ in matches)\n",
    "\n",
    "def check_full_text_match(doc, label_prefixes):\n",
    "    \"\"\"\n",
    "    Verifica si un match cubre TODO el texto (ignorando espacios).\n",
    "    label_prefixes: lista de prefijos de labels a buscar (ej: [\"METADATA_\", \"NEUTRAL_\"])\n",
    "    \"\"\"\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # Tokens que no son espacios\n",
    "    non_space_tokens = [i for i, token in enumerate(doc) if not token.is_space]\n",
    "\n",
    "    if not non_space_tokens:\n",
    "        return False\n",
    "\n",
    "    first_token = non_space_tokens[0]\n",
    "    last_token = non_space_tokens[-1]\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "\n",
    "        # Verificar si el label comienza con alguno de los prefijos\n",
    "        if any(label.startswith(prefix) for prefix in label_prefixes):\n",
    "            # El match debe cubrir desde el primer token no-espacio hasta el último\n",
    "            if start <= first_token and end > last_token:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def check_start_match(doc, label_prefixes):\n",
    "    \"\"\"\n",
    "    Verifica si hay un match al inicio del texto.\n",
    "    Útil para patrones como \"lanzamiento...\", \"cuenta regresiva...\", etc.\n",
    "    \"\"\"\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # Primer token no-espacio\n",
    "    non_space_tokens = [i for i, token in enumerate(doc) if not token.is_space]\n",
    "    if not non_space_tokens:\n",
    "        return False\n",
    "\n",
    "    first_token = non_space_tokens[0]\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "\n",
    "        if any(label.startswith(prefix) for prefix in label_prefixes):\n",
    "            # El match debe empezar en el primer token (o cerca)\n",
    "            if start <= first_token + 1:  # +1 para dar algo de margen\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES PRINCIPALES\n",
    "# ============================================================================\n",
    "\n",
    "def is_safe_non_pattern(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Valida que TODO el texto sea metadata, CTA neutral o info de lanzamiento.\n",
    "    Usa matchers de spaCy con validación de texto completo.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    if has_urgency_trigger(doc):\n",
    "        return False\n",
    "\n",
    "    # Verificar metadata (debe cubrir TODO el texto)\n",
    "    if check_full_text_match(doc, [\"METADATA_\"]):\n",
    "        return True\n",
    "\n",
    "    # Verificar CTAs neutrales (debe cubrir TODO el texto)\n",
    "    if check_full_text_match(doc, [\"NEUTRAL_\"]):\n",
    "        return True\n",
    "\n",
    "    # Verificar lanzamientos (puede ser todo el texto o empezar así)\n",
    "    if check_full_text_match(doc, [\"LAUNCH_\"]) or check_start_match(doc, [\"LAUNCH_\"]):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def is_anti_dark_fp(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detecta falsos positivos: eventos, sesiones técnicas, etc.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    if has_urgency_trigger(doc):\n",
    "        return False\n",
    "\n",
    "    # Detectar términos con phrase matcher\n",
    "    phrase_matches = phrase_matcher(doc, as_spans=False)\n",
    "\n",
    "    has_event = any(nlp.vocab.strings[match_id] == \"EVENT_TERMS\" for match_id, _, _ in phrase_matches)\n",
    "    has_start = any(nlp.vocab.strings[match_id] == \"EVENT_START\" for match_id, _, _ in phrase_matches)\n",
    "    has_tech_noun = any(nlp.vocab.strings[match_id] == \"TECH_NOUNS\" for match_id, _, _ in phrase_matches)\n",
    "    has_end_verb = any(nlp.vocab.strings[match_id] == \"END_VERBS\" for match_id, _, _ in phrase_matches)\n",
    "\n",
    "    # Eventos con inicio\n",
    "    if has_event and has_start:\n",
    "        return True\n",
    "\n",
    "    # Evento que comienza \"en X tiempo\"\n",
    "    if has_start and \" en \" in text_lower:\n",
    "        if re_in_time.search(text_lower) or re_clock.search(text_lower):\n",
    "            return True\n",
    "\n",
    "    # Tech nouns + end verbs + tiempo\n",
    "    if has_tech_noun and has_end_verb:\n",
    "        if re_in_time.search(text_lower) or re_clock.search(text_lower) or \" en \" in text_lower:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def prefilter_to_none(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Función principal de prefiltro.\n",
    "    Retorna True si el texto debe clasificarse como \"ninguno\" (no es dark pattern).\n",
    "\n",
    "    IMPORTANTE: Primero chequea patrones de shaming que DEBEN pasar adelante.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    # PRIORIDAD 1: Si tiene patrones de shaming, NO descartar (dejar pasar al modelo)\n",
    "    #if has_shaming_pattern(doc):\n",
    "    #    return False\n",
    "\n",
    "    # PRIORIDAD 2: Si no tiene shaming, aplicar filtros normales\n",
    "    return is_anti_dark_fp(text) or is_safe_non_pattern(text)\n",
    "\n",
    "def prepare_df(df):\n",
    "    df = df.copy()\n",
    "    df[\"binary\"] = np.where(df[\"type\"] == \"ninguno\", \"ninguno\", \"pattern\")\n",
    "    df[\"type\"] = df[\"type\"].str.strip().str.lower()\n",
    "    df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "    df[\"source\"] = df[\"source\"].astype(str).str.strip()\n",
    "\n",
    "    valid_types = {\"fake_urgency\", \"fake_scarcity\", \"shaming\", \"ninguno\"}\n",
    "    df = df[df[\"type\"].isin(valid_types)]\n",
    "    df = df[(df[\"content\"] != \"\") & (df[\"source\"] != \"\")].reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5a07c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de ENTRENAMIENTO\n",
    "df_train = pd.read_csv(\n",
    "    \"datasets/training_dataset.csv\",\n",
    "    header=None,\n",
    "    names=[\"type\", \"content\", \"source\"],\n",
    "    dtype=str\n",
    ").dropna()\n",
    "\n",
    "# Dataset de PRUEBA / EVALUACIÓN \n",
    "df_test = pd.read_csv(\n",
    "    \"datasets/unified_dataset.csv\",\n",
    "    header=None,\n",
    "    names=[\"type\", \"content\", \"source\"],\n",
    "    dtype=str\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1 (TRAIN CV): MODEL ONLY ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ninguno      0.542     0.371     0.441        35\n",
      "     pattern      0.869     0.930     0.898       157\n",
      "\n",
      "    accuracy                          0.828       192\n",
      "   macro avg      0.705     0.651     0.670       192\n",
      "weighted avg      0.809     0.828     0.815       192\n",
      "\n",
      "\n",
      "=== STAGE 1 (TRAIN CV): SYSTEM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ninguno      0.533     0.457     0.492        35\n",
      "     pattern      0.883     0.911     0.897       157\n",
      "\n",
      "    accuracy                          0.828       192\n",
      "   macro avg      0.708     0.684     0.694       192\n",
      "weighted avg      0.819     0.828     0.823       192\n",
      "\n",
      "\n",
      "Descartados por prefilter (train): 8\n",
      "         type      source                            content\n",
      "fake_scarcity      bidcom                    (6 disponibles)\n",
      "fake_scarcity        boca Stock disponible: (19 disponibles)\n",
      "fake_scarcity        etsy                     En 18 carritos\n",
      "      ninguno  aprendoseo                Quiero aprender SEO\n",
      "      ninguno  aprendoseo                   Quiero saber más\n",
      "      ninguno  aprendoseo                    Quiero más info\n",
      "      ninguno    LaNacion                   Quiero mi código\n",
      "      ninguno JuanMenorio     Quiero una consultoría digital\n",
      "\n",
      "=== ERRORES STAGE 1 (TRAIN CV – MODEL ONLY) ===\n",
      "Total errores: 33\n",
      "\n",
      "--- FALSOS NEGATIVOS ---\n",
      "[fake_scarcity | etsy] En más de 20 carritos\n",
      "[fake_scarcity | glowetta] Juan P. acaba de comprar Aceite de orégano PACK 2 hace 5 minutos\n",
      "[fake_urgency | nord_vpn] Su estado: Desprotegido -69 % de descuento + 3 meses extra 00 : 09 : 37 : 34\n",
      "[fake_urgency | rappi] ¡Tenés 30 días de entregas gratis!\n",
      "[fake_urgency | temu] -29% últimos 2 días\n",
      "[fake_urgency | temu] Ventas flash de fin de smeana\n",
      "[shaming | Desconocido] Olvídalo, quiero juegos geniales\n",
      "[shaming | Desconocido] No quiero aprender\n",
      "[shaming | NeilPatel] Si, quiero más tráfico\n",
      "[shaming | TreatsPlease] No quiero ser saludable, cancelar mi suscripción\n",
      "[fake_urgency | Desconocido] ¡Obten 10% de descuento entre tu primera orden! Codigo:FIRST10\n",
      "\n",
      "--- FALSOS POSITIVOS ---\n",
      "[ninguno | wish] 4 colores, 5 piezas\n",
      "[ninguno | wish] Hasta 27% de descuento\n",
      "[ninguno | temu] ARS2.100 de crédito por retraso\n",
      "[ninguno | temu] Llega a AR en tan solo 3 días hábiles después del envío\n",
      "[ninguno | aprendoseo] Quiero aprender SEO\n",
      "[ninguno | aprendoseo] Quiero saber más\n",
      "[ninguno | aprendoseo] Quiero más info\n",
      "[ninguno | Delish] Mostrame 14 recetas simples\n",
      "[ninguno | Desconocido] Cancelar mi suscripción\n",
      "[ninguno | Desconocido] No quiero suscribirme en este momento\n",
      "[ninguno | IA] Clase de yoga termina en 10 minutos\n",
      "[ninguno | IA] La inscripción al curso cierra en 3 días\n",
      "[ninguno | IA] recibe un regalo extra\n",
      "[ninguno | IA] Tu sesión caducará en 1 minuto\n",
      "[ninguno | IA] ¡Participa ya!\n",
      "[ninguno | IA] gana descuentos exclusivos\n",
      "[ninguno | IA] Últimas unidades disponibles\n",
      "[ninguno | Desconocido] Tu pedido expirará en 11:38\n",
      "[ninguno | Desconocido] Recibí una alerta cuando esté disponible\n",
      "[ninguno | Desconocido] Sorteo empieza\n",
      "[ninguno | Desconocido] Oferta extendida solo 2 días\n",
      "[ninguno | twitch] Siguiente stream en\n",
      "\n",
      "=== ERRORES STAGE 1 (TRAIN CV – SYSTEM) ===\n",
      "Total errores: 33\n",
      "\n",
      "--- FALSOS NEGATIVOS ---\n",
      "[fake_scarcity | bidcom] (6 disponibles)\n",
      "[fake_scarcity | boca] Stock disponible: (19 disponibles)\n",
      "[fake_scarcity | etsy] En más de 20 carritos\n",
      "[fake_scarcity | etsy] En 18 carritos\n",
      "[fake_scarcity | glowetta] Juan P. acaba de comprar Aceite de orégano PACK 2 hace 5 minutos\n",
      "[fake_urgency | nord_vpn] Su estado: Desprotegido -69 % de descuento + 3 meses extra 00 : 09 : 37 : 34\n",
      "[fake_urgency | rappi] ¡Tenés 30 días de entregas gratis!\n",
      "[fake_urgency | temu] -29% últimos 2 días\n",
      "[fake_urgency | temu] Ventas flash de fin de smeana\n",
      "[shaming | Desconocido] Olvídalo, quiero juegos geniales\n",
      "[shaming | Desconocido] No quiero aprender\n",
      "[shaming | NeilPatel] Si, quiero más tráfico\n",
      "[shaming | TreatsPlease] No quiero ser saludable, cancelar mi suscripción\n",
      "[fake_urgency | Desconocido] ¡Obten 10% de descuento entre tu primera orden! Codigo:FIRST10\n",
      "\n",
      "--- FALSOS POSITIVOS ---\n",
      "[ninguno | wish] 4 colores, 5 piezas\n",
      "[ninguno | wish] Hasta 27% de descuento\n",
      "[ninguno | temu] ARS2.100 de crédito por retraso\n",
      "[ninguno | temu] Llega a AR en tan solo 3 días hábiles después del envío\n",
      "[ninguno | Delish] Mostrame 14 recetas simples\n",
      "[ninguno | Desconocido] Cancelar mi suscripción\n",
      "[ninguno | Desconocido] No quiero suscribirme en este momento\n",
      "[ninguno | IA] Clase de yoga termina en 10 minutos\n",
      "[ninguno | IA] La inscripción al curso cierra en 3 días\n",
      "[ninguno | IA] recibe un regalo extra\n",
      "[ninguno | IA] Tu sesión caducará en 1 minuto\n",
      "[ninguno | IA] ¡Participa ya!\n",
      "[ninguno | IA] gana descuentos exclusivos\n",
      "[ninguno | IA] Últimas unidades disponibles\n",
      "[ninguno | Desconocido] Tu pedido expirará en 11:38\n",
      "[ninguno | Desconocido] Recibí una alerta cuando esté disponible\n",
      "[ninguno | Desconocido] Sorteo empieza\n",
      "[ninguno | Desconocido] Oferta extendida solo 2 días\n",
      "[ninguno | twitch] Siguiente stream en\n",
      "\n",
      "=== STAGE 2 (TRAIN CV): MODEL ONLY ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "fake_scarcity      0.809     0.792     0.800        48\n",
      " fake_urgency      0.833     0.890     0.861        73\n",
      "      shaming      1.000     0.889     0.941        36\n",
      "\n",
      "     accuracy                          0.860       157\n",
      "    macro avg      0.881     0.857     0.867       157\n",
      " weighted avg      0.864     0.860     0.861       157\n",
      "\n",
      "\n",
      "=== ERRORES STAGE 2 (TRAIN CV) ===\n",
      "Total errores: 22\n",
      "[bbriw_cosmetics] true=fake_scarcity pred=fake_urgency | Se vendieron 20 productos en las últimas 6 horas\n",
      "[bozoom] true=fake_scarcity pred=fake_urgency | Vendido hace 57 minutos\n",
      "[ebay] true=fake_scarcity pred=fake_urgency | Último\n",
      "[etsy] true=fake_scarcity pred=fake_urgency | En más de 20 carritos\n",
      "[etsy] true=fake_scarcity pred=fake_urgency | Quedan 2 unidades y una está en el carrito de otra persona\n",
      "[etsy] true=fake_scarcity pred=fake_urgency | En 18 carritos\n",
      "[falabella] true=fake_urgency pred=fake_scarcity | SOLO x 24h 18 : 21 : 52\n",
      "[gadnic] true=fake_urgency pred=fake_scarcity | ¡NO TE LAS PIERDAS! QUEDAN: 00 : 36 : 29 Horas Mins Seg\n",
      "[glowetta] true=fake_scarcity pred=fake_urgency | Juan P. acaba de comprar Aceite de orégano PACK 2 hace 5 minutos\n",
      "[kissmetric] true=fake_urgency pred=fake_scarcity | Solo quedan pocos cupos. 02:53\n",
      "[mercado_libre] true=fake_urgency pred=fake_scarcity | ¡NO TE LO PIERDAS! QUEDAN 00 : 14 : 58 : 13 DE DESCUENTOS 9/9\n",
      "[temu] true=fake_scarcity pred=fake_urgency | QUEDA(N) 32\n",
      "[pazprofunda] true=fake_urgency pred=fake_scarcity | Solo hoy 06 enero\n",
      "[spotify] true=fake_urgency pred=fake_scarcity | Última oportunidad\n",
      "[temu] true=fake_scarcity pred=fake_urgency | CASI AGOTADO(S)\n",
      "[tienda_clic] true=fake_scarcity pred=fake_urgency | ADVERTENCIA: ¡ÚLTIMOS ARTÍCULOS EN INVENTARIO!\n",
      "[Baeldung] true=shaming pred=fake_urgency | Continuar sin apoyarnos\n",
      "[eBook] true=shaming pred=fake_urgency | Reject the ebook\n",
      "[Desconocido] true=shaming pred=fake_scarcity | Soy una mala persona\n",
      "[Desconocido] true=shaming pred=fake_urgency | Si, amo las ofertas\n",
      "[Desconocido] true=fake_urgency pred=fake_scarcity | Disponible únicamente durante una semana\n",
      "[Desconocido] true=fake_urgency pred=fake_scarcity | Fijá el precio más bajo\n",
      "\n",
      "=== FINAL TEST: MODEL ONLY ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ninguno      0.935     0.729     0.819        59\n",
      "     pattern      0.906     0.981     0.942       157\n",
      "\n",
      "    accuracy                          0.912       216\n",
      "   macro avg      0.920     0.855     0.880       216\n",
      "weighted avg      0.914     0.912     0.908       216\n",
      "\n",
      "\n",
      "=== FINAL TEST: SYSTEM (PREFILTER + MODEL) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ninguno      0.908     1.000     0.952        59\n",
      "     pattern      1.000     0.962     0.981       157\n",
      "\n",
      "    accuracy                          0.972       216\n",
      "   macro avg      0.954     0.981     0.966       216\n",
      "weighted avg      0.975     0.972     0.973       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = prepare_df(df_train)\n",
    "df_test = prepare_df(df_test)\n",
    "df_test = df_test.sample(frac=0.2, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# STAGE 1 – TRAIN / CV\n",
    "# =========================\n",
    "\n",
    "X = df_train[\"content\"]\n",
    "y = df_train[\"binary\"]\n",
    "groups = df_train[\"source\"]\n",
    "\n",
    "pipeline_stage1 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
    "    (\"clf\", LogisticRegression(max_iter=3000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "pipeline_stage2 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "gkf1 = GroupKFold(n_splits=5)\n",
    "oof_model_stage1 = np.empty(len(df_train), dtype=object)\n",
    "oof_system_stage1 = np.empty(len(df_train), dtype=object)\n",
    "discarded = np.zeros(len(df_train), dtype=bool)\n",
    "\n",
    "for tr, te in gkf1.split(X, y, groups):\n",
    "    pipeline_stage1.fit(X.iloc[tr], y.iloc[tr])\n",
    "\n",
    "    te_texts = X.iloc[te]\n",
    "    preds_model = pipeline_stage1.predict(te_texts)\n",
    "    oof_model_stage1[te] = preds_model\n",
    "\n",
    "    disc = te_texts.apply(lambda x: prefilter_to_none(x)).values\n",
    "    discarded[te] = disc\n",
    "\n",
    "    preds_sys = preds_model.copy()\n",
    "    preds_sys[disc] = \"ninguno\"\n",
    "    oof_system_stage1[te] = preds_sys\n",
    "\n",
    "print(\"=== STAGE 1 (TRAIN CV): MODEL ONLY ===\")\n",
    "print(classification_report(y, oof_model_stage1, digits=3, zero_division=0))\n",
    "\n",
    "print(\"\\n=== STAGE 1 (TRAIN CV): SYSTEM ===\")\n",
    "print(classification_report(y, oof_system_stage1, digits=3, zero_division=0))\n",
    "\n",
    "print(\"\\nDescartados por prefilter (train):\", int(discarded.sum()))\n",
    "print(df_train.loc[discarded, [\"type\", \"source\", \"content\"]].to_string(index=False))\n",
    "\n",
    "df_err1 = df_train.copy()\n",
    "df_err1[\"y_true\"] = y.values\n",
    "df_err1[\"y_pred_model\"] = oof_model_stage1\n",
    "df_err1[\"y_pred_system\"] = oof_system_stage1\n",
    "\n",
    "err_model = df_err1[df_err1[\"y_true\"] != df_err1[\"y_pred_model\"]]\n",
    "err_sys = df_err1[df_err1[\"y_true\"] != df_err1[\"y_pred_system\"]]\n",
    "\n",
    "print(\"\\n=== ERRORES STAGE 1 (TRAIN CV – MODEL ONLY) ===\")\n",
    "print(f\"Total errores: {len(err_model)}\")\n",
    "print(\"\\n--- FALSOS NEGATIVOS ---\")\n",
    "for _, r in err_model[(err_model[\"y_true\"] == \"pattern\") & (err_model[\"y_pred_model\"] == \"ninguno\")].iterrows():\n",
    "    print(f\"[{r['type']} | {r['source']}] {r['content']}\")\n",
    "\n",
    "print(\"\\n--- FALSOS POSITIVOS ---\")\n",
    "for _, r in err_model[(err_model[\"y_true\"] == \"ninguno\") & (err_model[\"y_pred_model\"] == \"pattern\")].iterrows():\n",
    "    print(f\"[{r['type']} | {r['source']}] {r['content']}\")\n",
    "\n",
    "print(\"\\n=== ERRORES STAGE 1 (TRAIN CV – SYSTEM) ===\")\n",
    "print(f\"Total errores: {len(err_sys)}\")\n",
    "print(\"\\n--- FALSOS NEGATIVOS ---\")\n",
    "for _, r in err_sys[(err_sys[\"y_true\"] == \"pattern\") & (err_sys[\"y_pred_system\"] == \"ninguno\")].iterrows():\n",
    "    print(f\"[{r['type']} | {r['source']}] {r['content']}\")\n",
    "\n",
    "print(\"\\n--- FALSOS POSITIVOS ---\")\n",
    "for _, r in err_sys[(err_sys[\"y_true\"] == \"ninguno\") & (err_sys[\"y_pred_system\"] == \"pattern\")].iterrows():\n",
    "    print(f\"[{r['type']} | {r['source']}] {r['content']}\")\n",
    "\n",
    "# =========================\n",
    "# STAGE 2 – TRAIN / CV\n",
    "# =========================\n",
    "\n",
    "df_p = df_train[df_train[\"type\"] != \"ninguno\"].reset_index(drop=True)\n",
    "\n",
    "X2 = df_p[\"content\"]\n",
    "y2 = df_p[\"type\"]\n",
    "groups2 = df_p[\"source\"]\n",
    "\n",
    "gkf2 = GroupKFold(n_splits=5)\n",
    "oof_stage2 = np.empty(len(df_p), dtype=object)\n",
    "\n",
    "for tr, te in gkf2.split(X2, y2, groups2):\n",
    "    pipeline_stage2.fit(X2.iloc[tr], y2.iloc[tr])\n",
    "    oof_stage2[te] = pipeline_stage2.predict(X2.iloc[te])\n",
    "\n",
    "print(\"\\n=== STAGE 2 (TRAIN CV): MODEL ONLY ===\")\n",
    "print(classification_report(y2, oof_stage2, digits=3, zero_division=0))\n",
    "\n",
    "df_err2 = df_p.copy()\n",
    "df_err2[\"y_true\"] = y2.values\n",
    "df_err2[\"y_pred\"] = oof_stage2\n",
    "df_err2 = df_err2[df_err2[\"y_true\"] != df_err2[\"y_pred\"]]\n",
    "\n",
    "print(\"\\n=== ERRORES STAGE 2 (TRAIN CV) ===\")\n",
    "print(f\"Total errores: {len(df_err2)}\")\n",
    "for _, r in df_err2.iterrows():\n",
    "    print(f\"[{r['source']}] true={r['y_true']} pred={r['y_pred']} | {r['content']}\")\n",
    "\n",
    "# =========================\n",
    "# FIT FINAL MODELS (TRAIN ONLY)\n",
    "# =========================\n",
    "\n",
    "pipeline_stage1.fit(df_train[\"content\"], df_train[\"binary\"])\n",
    "pipeline_stage2.fit(df_p[\"content\"], df_p[\"type\"])\n",
    "\n",
    "# =========================\n",
    "# FINAL TEST (REAL EVAL)\n",
    "# =========================\n",
    "\n",
    "X_test = df_test[\"content\"]\n",
    "y_test = df_test[\"binary\"]\n",
    "\n",
    "preds_model_test = pipeline_stage1.predict(X_test)\n",
    "\n",
    "disc_test = X_test.apply(lambda x: prefilter_to_none(x)).values\n",
    "preds_system_test = preds_model_test.copy()\n",
    "preds_system_test[disc_test] = \"ninguno\"\n",
    "\n",
    "print(\"\\n=== FINAL TEST: MODEL ONLY ===\")\n",
    "print(classification_report(y_test, preds_model_test, digits=3, zero_division=0))\n",
    "\n",
    "print(\"\\n=== FINAL TEST: SYSTEM (PREFILTER + MODEL) ===\")\n",
    "print(classification_report(y_test, preds_system_test, digits=3, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

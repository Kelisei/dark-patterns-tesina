{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9e10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "NLP = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "from DarkPatternPredictor import DarkStrategy\n",
    "\n",
    "dark_matcher = Matcher(NLP.vocab)\n",
    "anti_dark_matcher = Matcher(NLP.vocab)\n",
    "\n",
    "URGENCY_TRIGGERS = [\n",
    "    \"apurate\",\n",
    "    \"ya\",\n",
    "    \"no te lo pierdas\",\n",
    "    \"última,\"\n",
    "    \"oportunidad\",\n",
    "    \"comprá\",\n",
    "    \"reservá\",\n",
    "    \"oferta\",\n",
    "    \"últimas\",\n",
    "    \"flash\",\n",
    "    \"sale\" ,\n",
    "    \"relámpago\",\n",
    "    \"aprovecha\"\n",
    "]\n",
    "\n",
    "TECH_NOUNS = [\n",
    "    \"sesión\", \"batería\", \"dispositivo\", \"equipo\", \"sistema\",\n",
    "    \"conexión\", \"proceso\", \"operación\", \"pantalla\", \"aplicación\",\n",
    "    \"instancia\", \"entorno\"\n",
    "]\n",
    "\n",
    "END_VERBS = [\n",
    "    \"expirar\", \"caducar\", \"vencer\", \"cerrar\",\n",
    "    \"finalizar\", \"terminar\", \"agotarse\",\n",
    "    \"apagarse\", \"desconectarse\", \"bloquearse\"\n",
    "]\n",
    "\n",
    "TIME_UNITS = [\n",
    "    \"segundo\", \"segundos\",\n",
    "    \"minuto\", \"minutos\",\n",
    "    \"hora\", \"horas\",\n",
    "    \"s\", \"m\", \"h\"\n",
    "]\n",
    "\n",
    "def get_patterns():\n",
    "    return {\n",
    "        #SHAMING STRATEGIES\n",
    "\n",
    "        # Primera persona\n",
    "        \"FP_VERB\": [[{\"POS\": \"VERB\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}]],\n",
    "        \"FP_COPULA\": [[{\"DEP\": \"cop\", \"POS\": \"AUX\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}]],\n",
    "        \"FP_ME_VERB\": [[{\"POS\": \"PRON\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}, {\"POS\": \"VERB\"}]],\n",
    "\n",
    "        # Perífrasis\n",
    "        \"FP_PERIFRASIS_VOY_A\": [[\n",
    "            {\"DEP\": \"aux\", \"POS\": \"AUX\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}},\n",
    "            {\"DEP\": \"mark\", \"POS\": \"ADP\"},\n",
    "            {\"POS\": \"VERB\"},\n",
    "        ]],\n",
    "\n",
    "        # Ser desordenado es lo mío\n",
    "        \"FP_ES_LO_MIO\": [[\n",
    "            {\"LEMMA\": {\"IN\": [\"seguir\", \"ignorar\", \"ser\", \"hacer\"]}, \"POS\": {\"IN\": [\"VERB\", \"AUX\"]}},\n",
    "            {\"OP\": \"+\", \"POS\": {\"NOT_IN\": [\"PUNCT\"]}},\n",
    "            {\"LEMMA\": \"ser\", \"POS\": {\"IN\": [\"AUX\", \"VERB\"]}},\n",
    "            {\"LOWER\": \"lo\"},\n",
    "            {\"LOWER\": \"mío\"},\n",
    "        ]],\n",
    "\n",
    "        # Ironía\n",
    "        \"IRONIA_PREFIERO_NO\": [[{\"LEMMA\": \"preferir\", \"POS\": \"VERB\"}, {\"LOWER\": \"no\"}, {\"POS\": \"VERB\"}]],\n",
    "        \"IRONIA_QUIEN_NECESITA\": [[{\"LOWER\": \"quién\"}, {\"LEMMA\": \"necesitar\", \"POS\": \"VERB\"}]],\n",
    "        \"IRONIA_PORQUE_HABRIA_DE\": [[\n",
    "            {\"LOWER\": \"por\"}, {\"LOWER\": \"qué\"}, {\"LEMMA\": \"haber\", \"POS\": \"AUX\"}, {\"LOWER\": \"de\"}, {\"POS\": \"VERB\"}\n",
    "        ]],\n",
    "\n",
    "        # Metáforas\n",
    "        \"META_VERBOS_ES_MI\": [[\n",
    "            {\"LEMMA\": {\"IN\": [\"ignorar\", \"vivir\", \"ser\", \"estar\", \"perder\", \"arruinar\", \"hacer\", \"rechazar\", \"fracasar\", \"seguir\"]}},\n",
    "            {\"OP\": \"*\"},\n",
    "            {\"LOWER\": \"es\"},\n",
    "            {\"LOWER\": \"mi\"},\n",
    "            {\"OP\": \"+\"}\n",
    "        ]],\n",
    "    }\n",
    "\n",
    "def get_anti_patterns():\n",
    "    return {\n",
    "        \"INFO_EVENT\": [[\n",
    "            {\"LEMMA\": {\"IN\": [\"comenzar\", \"empezar\", \"iniciar\"]}},\n",
    "            {\"LOWER\": \"en\"},\n",
    "            {\"LIKE_NUM\": True, \"OP\": \"?\"},\n",
    "            {\"LOWER\": {\"IN\": [\"minutos\", \"horas\", \"segundos\"]}}\n",
    "        ]],\n",
    "\n",
    "        \"STREAMING\": [[\n",
    "            {\"LOWER\": {\"IN\": [\"streaming\", \"clase\", \"evento\", \"live\"]}},\n",
    "            {\"LOWER\": {\"IN\": [\"empieza\", \"comienza\", \"inicia\"]}, \"OP\": \"?\"},\n",
    "            {\"LOWER\": \"en\", \"OP\": \"?\"},\n",
    "            {\"LIKE_NUM\": True, \"OP\": \"?\"}\n",
    "        ]],\n",
    "\n",
    "        \"TECH_TIMEOUT_ABSTRACT\": [[\n",
    "            {\"LOWER\": {\"IN\": [\"tu\", \"su\", \"la\", \"el\"]}, \"OP\": \"?\"},\n",
    "            {\"LEMMA\": {\"IN\": TECH_NOUNS}},\n",
    "            {\"POS\": {\"IN\": [\"AUX\", \"VERB\"]}, \"OP\": \"*\"},\n",
    "            {\"LEMMA\": {\"IN\": END_VERBS}},\n",
    "            {\"LOWER\": \"en\", \"OP\": \"?\"},\n",
    "            {\"LIKE_NUM\": True, \"OP\": \"?\"},\n",
    "            {\"LOWER\": {\"IN\": TIME_UNITS}, \"OP\": \"?\"}\n",
    "        ]]\n",
    "    }\n",
    "\n",
    "def init_matchers():\n",
    "    # Dark patterns (si los usás)\n",
    "    for name, patterns in get_patterns().items():\n",
    "        dark_matcher.add(name, patterns)\n",
    "\n",
    "    # Anti-patterns (DESCARTE)\n",
    "    for name, patterns in get_anti_patterns().items():\n",
    "        anti_dark_matcher.add(name, patterns)\n",
    "\n",
    "init_matchers()\n",
    "\n",
    "def has_urgency_trigger(doc):\n",
    "    text = doc.text.lower()\n",
    "    return any(trigger in text for trigger in URGENCY_TRIGGERS)\n",
    "\n",
    "def is_anti_dark_fp(text):\n",
    "    doc = NLP(text)\n",
    "\n",
    "    # Regla de oro: si hay lenguaje de urgencia, NO descartar\n",
    "    if has_urgency_trigger(doc):\n",
    "        return False\n",
    "\n",
    "    matches = anti_dark_matcher(doc)\n",
    "    return len(matches) > 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b8df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: pattern vs ninguno ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ninguno      0.800     0.211     0.333        38\n",
      "     pattern      0.829     0.986     0.901       147\n",
      "\n",
      "    accuracy                          0.827       185\n",
      "   macro avg      0.814     0.598     0.617       185\n",
      "weighted avg      0.823     0.827     0.784       185\n",
      "\n",
      "Descartados por NLP (stage 0): 5\n",
      "\n",
      "=== STAGE 2: urgency vs scarcity vs shaming ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "fake_scarcity      0.830     0.812     0.821        48\n",
      " fake_urgency      0.824     0.889     0.855        63\n",
      "      shaming      1.000     0.889     0.941        36\n",
      "\n",
      "     accuracy                          0.864       147\n",
      "    macro avg      0.884     0.863     0.872       147\n",
      " weighted avg      0.869     0.864     0.865       147\n",
      "\n",
      "\n",
      "Total errores Stage 1: 32\n",
      "\n",
      "=== FALSOS NEGATIVOS (pattern → ninguno) ===\n",
      "- [Desconocido] Olvídalo, quiero juegos geniales\n",
      "- [NeilPatel] Si, quiero más tráfico\n",
      "\n",
      "=== FALSOS POSITIVOS (ninguno → pattern) ===\n",
      "- [wish] 4 colores, 5 piezas\n",
      "- [wish] 4 colores\n",
      "- [wish] Hasta 27% de descuento\n",
      "- [wish] 8 tamaños\n",
      "- [temu] ARS2.100 de crédito por retraso\n",
      "- [temu] Llega a AR en tan solo 3 días hábiles después del envío\n",
      "- [temu] 558 ventas\n",
      "- [Desconocido] No gracias\n",
      "- [aprendoseo] Quiero aprender SEO\n",
      "- [aprendoseo] Quiero saber más\n",
      "- [aprendoseo] Quiero más info\n",
      "- [LaNacion] Quiero mi código\n",
      "- [JuanMenorio] Quiero una consultoría digital\n",
      "- [Amazon] Si, me voy a quedar el producto\n",
      "- [Amazon] No, lo quiero devolver\n",
      "- [CocaCola] Si por favor\n",
      "- [Desconocido] Descubrirlo ahora\n",
      "- [Delish] Mostrame 14 recetas simples\n",
      "- [Desconocido] Si, mandenmé alertas de stock\n",
      "- [eBook] Si, obtener el ebook\n",
      "- [eBook] Descargar ahora\n",
      "- [Desconocido] Cancelar mi suscripción\n",
      "- [Desconocido] No quiero suscribirme en este momento\n",
      "- [niacinamide] He leído y acepto la política de privacidad\n",
      "- [IA] La inscripción al curso cierra en 3 días\n",
      "- [IA] La batería del dispositivo se agotará en 2h\n",
      "- [IA] recibe un regalo extra\n",
      "- [IA] ¡Participa ya!\n",
      "- [IA] gana descuentos exclusivos\n",
      "- [IA] Últimas unidades disponibles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "df = pd.read_csv(\"datasets/unified_dataset.csv\").dropna(\n",
    "    subset=[\"type\", \"content\", \"source\"]\n",
    ")\n",
    "\n",
    "df[\"binary\"] = df[\"type\"].apply(\n",
    "    lambda x: \"pattern\" if x in [\n",
    "        \"fake_urgency\", \"fake_scarcity\", \"shaming\"\n",
    "    ] else \"ninguno\"\n",
    ")\n",
    "\n",
    "X = df[\"content\"]\n",
    "y = df[\"binary\"]\n",
    "groups = df[\"source\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 1 — PATTERN vs NINGUNO (WITH NLP FILTER)\n",
    "# ============================================================\n",
    "\n",
    "pipeline_stage1 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=1,\n",
    "        max_df=0.95\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=3000,\n",
    "        class_weight={\"pattern\": 1.0, \"ninguno\": 2.0}\n",
    "    ))\n",
    "])\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "oof_pred = np.empty(len(df), dtype=object)\n",
    "discarded_by_nlp = []\n",
    "\n",
    "for tr, te in gkf.split(X, y, groups):\n",
    "    pipeline_stage1.fit(X.iloc[tr], y.iloc[tr])\n",
    "    preds = pipeline_stage1.predict(X.iloc[te])\n",
    "\n",
    "    for i, idx in enumerate(te):\n",
    "        text = X.iloc[idx]\n",
    "\n",
    "        if is_anti_dark_fp(text):\n",
    "            oof_pred[idx] = \"ninguno\"\n",
    "            discarded_by_nlp.append(text)\n",
    "        else:\n",
    "            oof_pred[idx] = preds[i]\n",
    "\n",
    "\n",
    "print(\"=== STAGE 1: pattern vs ninguno ===\")\n",
    "print(classification_report(y, oof_pred, digits=3))\n",
    "print(f\"Descartados por NLP (stage 0): {len(discarded_by_nlp)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 2 — MULTICLASS (solo patterns reales)\n",
    "# ============================================================\n",
    "\n",
    "df_p = df[df[\"type\"] != \"ninguno\"]\n",
    "\n",
    "X2 = df_p[\"content\"]\n",
    "y2 = df_p[\"type\"]\n",
    "groups2 = df_p[\"source\"]\n",
    "\n",
    "pipeline_stage2 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=1,\n",
    "        max_df=0.95\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "oof_pred2 = np.empty(len(df_p), dtype=object)\n",
    "\n",
    "for tr, te in gkf.split(X2, y2, groups2):\n",
    "    pipeline_stage2.fit(X2.iloc[tr], y2.iloc[tr])\n",
    "    oof_pred2[te] = pipeline_stage2.predict(X2.iloc[te])\n",
    "\n",
    "print(\"\\n=== STAGE 2: urgency vs scarcity vs shaming ===\")\n",
    "print(classification_report(y2, oof_pred2, digits=3))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ERROR ANALYSIS — STAGE 1\n",
    "# ============================================================\n",
    "\n",
    "df_err = df.copy()\n",
    "df_err[\"y_true\"] = y.values\n",
    "df_err[\"y_pred\"] = oof_pred\n",
    "\n",
    "df_err = df_err[df_err[\"y_true\"] != df_err[\"y_pred\"]]\n",
    "\n",
    "print(f\"\\nTotal errores Stage 1: {len(df_err)}\")\n",
    "\n",
    "fn = df_err[\n",
    "    (df_err[\"y_true\"] == \"pattern\") &\n",
    "    (df_err[\"y_pred\"] == \"ninguno\")\n",
    "]\n",
    "\n",
    "print(\"\\n=== FALSOS NEGATIVOS (pattern → ninguno) ===\")\n",
    "for _, r in fn.iterrows():\n",
    "    print(f\"- [{r['source']}] {r['content']}\")\n",
    "\n",
    "fp = df_err[\n",
    "    (df_err[\"y_true\"] == \"ninguno\") &\n",
    "    (df_err[\"y_pred\"] == \"pattern\")\n",
    "]\n",
    "\n",
    "print(\"\\n=== FALSOS POSITIVOS (ninguno → pattern) ===\")\n",
    "for _, r in fp.iterrows():\n",
    "    print(f\"- [{r['source']}] {r['content']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f3c38f551c689f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T16:43:06.269115Z",
     "start_time": "2026-01-10T16:43:01.439883900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =======================\n",
    "# MULTILABEL CELL (END-TO-END)\n",
    "# - Extra normalization: __DATE__ __PCT__ __MONEY__ __NUM__ + __SOCIAL_COUNT__\n",
    "# - Features: word+char TF-IDF\n",
    "# - Per-label thresholds (OOF) instead of predict() @0.5\n",
    "# - Prefilter updated: never discard if social proof obvious (__SOCIAL_COUNT__ or regex)\n",
    "# =======================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÓN DATASET (MULTILABEL)\n",
    "# ============================================================================\n",
    "df = pd.read_csv(\n",
    "    \"datasets/unified_dataset.csv\",\n",
    "    header=None,\n",
    "    names=[\"type\", \"content\", \"source\"],\n",
    "    dtype=str\n",
    ").dropna()\n",
    "\n",
    "df[\"type\"] = df[\"type\"].str.strip().str.lower()\n",
    "df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "df[\"source\"] = df[\"source\"].astype(str).str.strip()\n",
    "\n",
    "valid_base_labels = {\"fake_urgency\", \"fake_scarcity\", \"shaming\", \"ninguno\", \"social_proof\"}\n",
    "\n",
    "def parse_labels(t: str):\n",
    "    parts = [p.strip().lower() for p in str(t).split(\"|\") if p.strip()]\n",
    "    parts = [p for p in parts if p in valid_base_labels]\n",
    "    if not parts:\n",
    "        return [\"ninguno\"]\n",
    "    if \"ninguno\" in parts and len(parts) > 1:\n",
    "        parts = [p for p in parts if p != \"ninguno\"]\n",
    "        if not parts:\n",
    "            parts = [\"ninguno\"]\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "df[\"labels\"] = df[\"type\"].apply(parse_labels)\n",
    "df = df[(df[\"content\"] != \"\") & (df[\"source\"] != \"\")].reset_index(drop=True)\n",
    "\n",
    "# ============================================================================\n",
    "# NORMALIZACIÓN (TIMERS / STOCK / PEOPLE + DATE/NUM)\n",
    "# ============================================================================\n",
    "RE_SPACED_COLON_TIMER = re.compile(r\"\\b\\d{1,2}\\s*:\\s*\\d{1,2}(?:\\s*:\\s*\\d{1,2}){1,3}\\b\")\n",
    "RE_D_COLON_TIMER = re.compile(r\"\\b\\d+\\s*[dD]\\s*:\\s*\\d{1,2}(?:\\s*:\\s*\\d{1,2}){1,2}\\b\")\n",
    "RE_COLON_TIMER = re.compile(r\"\\b\\d{1,2}(?::\\d{1,2}){1,4}\\b\")\n",
    "RE_UNIT_TIMER = re.compile(\n",
    "    r\"(?ix)\\b(\"\n",
    "    r\"(?:\\d+\\s*(?:d|días?|dia|day|days))\\s*\"\n",
    "    r\"(?:\\d+\\s*(?:h|hs|hr|hrs|horas?))?\\s*\"\n",
    "    r\"(?:\\d+\\s*(?:m|min|mins|minutos?))?\\s*\"\n",
    "    r\"(?:\\d+\\s*(?:s|seg|segs|segundos?))?\"\n",
    "    r\"|\"\n",
    "    r\"(?:\\d+\\s*(?:h|hs|hr|hrs|horas?))\\s*\"\n",
    "    r\"(?:\\d+\\s*(?:m|min|mins|minutos?))\\s*\"\n",
    "    r\"(?:\\d+\\s*(?:s|seg|segs|segundos?))\"\n",
    "    r\")\\b\"\n",
    ")\n",
    "RE_CLOCK_ONLY = re.compile(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\")\n",
    "\n",
    "# extra\n",
    "RE_DATE = re.compile(r\"\\b(\\d{1,2}[/-]\\d{1,2})(?:[/-]\\d{2,4})?\\b\")  # 07/01, 07-01, 07/01/2026\n",
    "RE_PERCENT = re.compile(r\"\\b\\d{1,3}\\s*%\\b\")\n",
    "RE_CURRENCY = re.compile(r\"(?i)\\b(?:ars|\\$|usd|u\\$s|€)\\s*\\d+(?:[.,]\\d+)*\\b\")\n",
    "RE_STANDALONE_NUM = re.compile(r\"\\b\\d+(?:[.,]\\d+)*\\b\")\n",
    "\n",
    "RE_SOCIAL_UNITS = re.compile(\n",
    "    r\"(?i)\\b__num__\\s*(comprados?|vendidos?|pedidos?|ventas?|visit(as|os)?|vistas?|\"\n",
    "    r\"mirando|viendo|en\\s*carritos?|añadid[oa]s?|agregad[oa]s?)\\b\"\n",
    ")\n",
    "\n",
    "def normalize_placeholders(text: str, normalize_stock=True, normalize_people=True) -> str:\n",
    "    t = str(text)\n",
    "\n",
    "    # TIMERS -> __TIMER__\n",
    "    t = RE_SPACED_COLON_TIMER.sub(\"__TIMER__\", t)\n",
    "    t = RE_D_COLON_TIMER.sub(\"__TIMER__\", t)\n",
    "    t = RE_COLON_TIMER.sub(\"__TIMER__\", t)\n",
    "    t = RE_UNIT_TIMER.sub(\"__TIMER__\", t)\n",
    "    t = RE_CLOCK_ONLY.sub(\"__TIMER__\", t)\n",
    "\n",
    "    # STOCK -> __STOCK__\n",
    "    if normalize_stock:\n",
    "        t = re.sub(r\"(?i)\\b(qued[ae]n?|queda\\(n\\))\\s*\\d+\\b\", r\"\\1 __STOCK__\", t)\n",
    "        t = re.sub(r\"(?i)\\bsolo\\s*qued[ae]n?\\s*\\d+\\s*en\\s*stock\\b\", \"Solo quedan __STOCK__ en stock\", t)\n",
    "        t = re.sub(r\"(?i)\\(\\s*\\d+\\s*disponibles?\\s*\\)\", \"(__STOCK__ disponibles)\", t)\n",
    "        t = re.sub(r\"(?i)(stock\\s*disponible:\\s*)\\(\\s*\\d+\\s*disponibles?\\s*\\)\", r\"\\1(__STOCK__ disponibles)\", t)\n",
    "\n",
    "    # PEOPLE -> __PEOPLE__\n",
    "    if normalize_people:\n",
    "        t = re.sub(r\"(?i)\\b\\d+\\s*personas?\\b\", \"__PEOPLE__\", t)\n",
    "        t = re.sub(r\"(?i)\\ben\\s*m[aá]s\\s*de\\s*\\d+\\s*carritos\\b\", \"en __PEOPLE__ carritos\", t)\n",
    "        t = re.sub(r\"(?i)\\ben\\s*\\d+\\s*carritos\\b\", \"en __PEOPLE__ carritos\", t)\n",
    "\n",
    "    # scarcity keywords\n",
    "    t = re.sub(r\"(?i)\\b(casi\\s*agotad[oa]s?|stock\\s*bajo|[uú]ltimas?\\s*\\d+|[uú]ltimas?\\s*unidades|se\\s*agotará\\s*pronto|se\\s*agotar[aá]\\s*pronto)\\b\", \"__SCARCITY__\", t)\n",
    "    t = re.sub(r\"(?i)\\bsolo\\s*qued[ae]?\\b\", \"solo queda\", t)\n",
    "\n",
    "    # SCARCITY semántica (NO numérica)\n",
    "    t = re.sub(\n",
    "        r\"(?i)\\b(\"\n",
    "        r\"stock\\s+bajo|\"\n",
    "        r\"se\\s+agota(r[aá]|r[aá]n)?\\s+pronto|\"\n",
    "        r\"una\\s+vez\\s+que\\s+se\\s+agote\\s+se\\s+acab[oó]|\"\n",
    "        r\"vendi[eé]ndose\\s+r[aá]pido|\"\n",
    "        r\"se\\s+vende\\s+r[aá]pido|\"\n",
    "        r\"movi[eé]ndose\\s+r[aá]pido|\"\n",
    "        r\"alta\\s+demanda|\"\n",
    "        r\"en\\s+tu\\s+carrito\\s+se\\s+est[aá]\\s+agotando\"\n",
    "        r\")\\b\",\n",
    "        \"__SCARCITY__\",\n",
    "        t\n",
    "    )\n",
    "\n",
    "    # FECHAS -> __DATE__\n",
    "    t = RE_DATE.sub(\"__DATE__\", t)\n",
    "\n",
    "    # % -> __PCT__\n",
    "    t = RE_PERCENT.sub(\"__PCT__\", t)\n",
    "\n",
    "    # MONEDA -> __MONEY__\n",
    "    t = RE_CURRENCY.sub(\"__MONEY__\", t)\n",
    "\n",
    "    # NUM -> __NUM__ (después de timers/fechas/moneda)\n",
    "    t = RE_STANDALONE_NUM.sub(\"__NUM__\", t)\n",
    "\n",
    "    # SOCIAL PROOF compact (muy útil para textos cortos)\n",
    "    t = RE_SOCIAL_UNITS.sub(\"__SOCIAL_COUNT__\", t)\n",
    "\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "df[\"content_norm\"] = df[\"content\"].apply(normalize_placeholders)\n",
    "\n",
    "# Labels positivas\n",
    "POS_LABELS = [\"fake_urgency\", \"fake_scarcity\", \"shaming\", \"social_proof\"]\n",
    "\n",
    "Y = np.zeros((len(df), len(POS_LABELS)), dtype=int)\n",
    "for i, labs in enumerate(df[\"labels\"].tolist()):\n",
    "    for j, lab in enumerate(POS_LABELS):\n",
    "        if lab in labs:\n",
    "            Y[i, j] = 1\n",
    "\n",
    "X = df[\"content_norm\"]\n",
    "groups = df[\"source\"]\n",
    "\n",
    "# =========================\n",
    "# SOCIAL PROOF (regex)\n",
    "# =========================\n",
    "RE_SOCIAL_PROOF = re.compile(\n",
    "    r\"(?ix)\\b(\"\n",
    "    r\"__social_count__\"\n",
    "    r\"|(?:\\d+|__num__|__people__)\\s*(?:personas?)\\s*(?:est[aá]n\\s*)?(?:viendo|mirando)\\b\"\n",
    "    r\"|en\\s*(?:m[aá]s\\s*de\\s*)?(?:\\d+|__num__|__people__)\\s*carritos?\\b\"\n",
    "    r\"|acaba\\s*de\\s*comprar\\b\"\n",
    "    r\"|(?:vendid[oa]s?|comprad[oa]s?|pedidos?)\\b\"\n",
    "    r\")\"\n",
    ")\n",
    "\n",
    "def has_social_proof(text: str) -> bool:\n",
    "    return bool(RE_SOCIAL_PROOF.search(str(text).lower()))\n",
    "\n",
    "# ============================================================================\n",
    "# PREFILTER\n",
    "# ============================================================================\n",
    "URGENCY_TRIGGERS = [\n",
    "    \"apurate\", \"apúrate\", \"ya\", \"no te lo pierdas\", \"última\", \"ultima\", \"oportunidad\",\n",
    "    \"comprá\", \"compra\", \"reservá\", \"reserva\", \"oferta\", \"últimas\", \"ultimas\", \"flash\", \"sale\",\n",
    "    \"relámpago\", \"relampago\", \"aprovecha\", \"ahora o nunca\", \"por tiempo limitado\", \"ultimo día\",\n",
    "    \"último día\", \"última oportunidad\", \"ultima oportunidad\", \"solo hoy\", \"sólo hoy\",\n",
    "    \"solo ahora\", \"sólo ahora\", \"termina en\", \"finaliza en\", \"quedan\", \"queda\",\n",
    "    \"últimos\", \"ultimos\", \"stock bajo\", \"casi agotado\",\n",
    "    \"__timer__\", \"__stock__\",\n",
    "]\n",
    "\n",
    "TECH_NOUNS = [\n",
    "    \"sesión\", \"sesion\", \"batería\", \"bateria\", \"dispositivo\", \"equipo\", \"sistema\",\n",
    "    \"conexión\", \"conexion\", \"proceso\", \"operación\", \"operacion\", \"pantalla\",\n",
    "    \"aplicación\", \"aplicacion\", \"instancia\", \"entorno\", \"pedido\", \"token\"\n",
    "]\n",
    "\n",
    "END_VERBS = [\n",
    "    \"expira\", \"expirar\", \"caduca\", \"caducar\", \"vence\", \"vencer\", \"cierra\", \"cerrar\",\n",
    "    \"finaliza\", \"finalizar\", \"termina\", \"terminar\", \"se agotará\", \"se agotara\",\n",
    "    \"agotarse\", \"apaga\", \"apagarse\", \"desconecta\", \"desconectarse\", \"bloquea\", \"bloquearse\"\n",
    "]\n",
    "\n",
    "EVENT_TERMS = [\"clase\", \"evento\", \"live\", \"streaming\", \"stream\", \"partido\", \"examen\"]\n",
    "EVENT_START_VERBS = [\"empieza\", \"comienza\", \"inicia\", \"arranca\", \"comenzar\", \"empezar\", \"iniciar\"]\n",
    "\n",
    "re_in_time = re.compile(r\"\\b\\d+\\s*(?:segundos?|minutos?|horas?|hs|h|m|s|d[ií]as?|d[ií]a)\\b\", re.IGNORECASE)\n",
    "re_clock = re.compile(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "shaming_matcher = Matcher(nlp.vocab)\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "# --- SHAMING (prioridad: no descartar) ---\n",
    "shaming_matcher.add(\"FP_VERB\", [[{\"POS\": \"VERB\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}]])\n",
    "shaming_matcher.add(\"FP_COPULA\", [[{\"DEP\": \"cop\", \"POS\": \"AUX\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}]])\n",
    "shaming_matcher.add(\"FP_ME_VERB\", [[{\"POS\": \"PRON\", \"MORPH\": {\"IS_SUPERSET\": [\"Person=1\", \"Number=Sing\"]}}, {\"POS\": \"VERB\"}]])\n",
    "\n",
    "# --- METADATA / NEUTRAL / LAUNCH  ---\n",
    "matcher.add(\"METADATA_UNITS_FULL\", [[\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LIKE_NUM\": True},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": {\"IN\": [\"colores\", \"color\", \"tamaños\", \"tamaño\", \"talles\", \"talle\", \"piezas\", \"pieza\", \"unidades\", \"unidad\"]}},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "]])\n",
    "matcher.add(\"NEUTRAL_NO_THANKS_FULL\", [[\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"no\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"gracias\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "]])\n",
    "matcher.add(\"LAUNCH_AVAILABLE_SOON\", [[\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": \"disponible\"},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"},\n",
    "    {\"LOWER\": {\"IN\": [\"próximamente\", \"proximamente\"]}},\n",
    "    {\"IS_SPACE\": True, \"OP\": \"*\"}\n",
    "]])\n",
    "\n",
    "phrase_matcher.add(\"URGENCY_TRIGGERS\", [nlp.make_doc(t) for t in URGENCY_TRIGGERS])\n",
    "phrase_matcher.add(\"TECH_NOUNS\", [nlp.make_doc(t) for t in TECH_NOUNS])\n",
    "phrase_matcher.add(\"END_VERBS\", [nlp.make_doc(t) for t in END_VERBS])\n",
    "phrase_matcher.add(\"EVENT_TERMS\", [nlp.make_doc(t) for t in EVENT_TERMS])\n",
    "phrase_matcher.add(\"EVENT_START\", [nlp.make_doc(t) for t in EVENT_START_VERBS])\n",
    "\n",
    "def has_shaming_pattern(doc):\n",
    "    return len(shaming_matcher(doc)) > 0\n",
    "\n",
    "def has_urgency_trigger(doc):\n",
    "    matches = phrase_matcher(doc, as_spans=False)\n",
    "    return any(nlp.vocab.strings[mid] == \"URGENCY_TRIGGERS\" for mid, _, _ in matches)\n",
    "\n",
    "def check_full_text_match(doc, label_prefixes):\n",
    "    matches = matcher(doc)\n",
    "    non_space_tokens = [i for i, tok in enumerate(doc) if not tok.is_space]\n",
    "    if not non_space_tokens:\n",
    "        return False\n",
    "    first_token = non_space_tokens[0]\n",
    "    last_token = non_space_tokens[-1]\n",
    "    for match_id, start, end in matches:\n",
    "        lab = nlp.vocab.strings[match_id]\n",
    "        if any(lab.startswith(p) for p in label_prefixes):\n",
    "            if start <= first_token and end > last_token:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_safe_non_pattern(text: str) -> bool:\n",
    "    doc = nlp(text.lower())\n",
    "    if has_urgency_trigger(doc):\n",
    "        return False\n",
    "    if check_full_text_match(doc, [\"METADATA_\"]):\n",
    "        return True\n",
    "    if check_full_text_match(doc, [\"NEUTRAL_\"]):\n",
    "        return True\n",
    "    if check_full_text_match(doc, [\"LAUNCH_\"]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_anti_dark_fp(text: str) -> bool:\n",
    "    doc = nlp(text.lower())\n",
    "    text_lower = text.lower()\n",
    "    if has_urgency_trigger(doc):\n",
    "        return False\n",
    "\n",
    "    pm = phrase_matcher(doc, as_spans=False)\n",
    "    has_event = any(nlp.vocab.strings[mid] == \"EVENT_TERMS\" for mid, _, _ in pm)\n",
    "    has_start = any(nlp.vocab.strings[mid] == \"EVENT_START\" for mid, _, _ in pm)\n",
    "    has_tech = any(nlp.vocab.strings[mid] == \"TECH_NOUNS\" for mid, _, _ in pm)\n",
    "    has_end = any(nlp.vocab.strings[mid] == \"END_VERBS\" for mid, _, _ in pm)\n",
    "\n",
    "    if has_event and has_start:\n",
    "        return True\n",
    "\n",
    "    if has_start and \" en \" in text_lower:\n",
    "        if re_in_time.search(text_lower) or re_clock.search(text_lower):\n",
    "            return True\n",
    "\n",
    "    if has_tech and has_end:\n",
    "        if re_in_time.search(text_lower) or re_clock.search(text_lower) or \" en \" in text_lower:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def prefilter_to_none(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Multi-label:\n",
    "    - Si hay shaming => NO descartar\n",
    "    - Si hay social_proof => NO descartar\n",
    "    - Si es \"anti-dark\" (tech/event) o safe non-pattern => descartar (ninguno)\n",
    "    \"\"\"\n",
    "    t = str(text)\n",
    "\n",
    "    # blindaje por normalización: si detectaste __SOCIAL_COUNT__ => no descartar\n",
    "    if \"__social_count__\" in t.lower():\n",
    "        return False\n",
    "\n",
    "    doc = nlp(t.lower())\n",
    "    if has_shaming_pattern(doc):\n",
    "        return False\n",
    "\n",
    "    if has_social_proof(t):\n",
    "        return False\n",
    "\n",
    "    return is_anti_dark_fp(t) or is_safe_non_pattern(t)\n",
    "\n",
    "word = TfidfVectorizer(ngram_range=(1, 3), min_df=2, max_df=0.95)\n",
    "char = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), min_df=2)\n",
    "tfidf = FeatureUnion([(\"word\", word), (\"char\", char)])\n",
    "\n",
    "pipeline_ml = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", OneVsRestClassifier(LogisticRegression(\n",
    "        max_iter=4000,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"liblinear\"\n",
    "    )))\n",
    "])\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "oof_proba = np.zeros((len(df), len(POS_LABELS)), dtype=float)\n",
    "discarded = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "for tr, te in gkf.split(X, Y, groups):\n",
    "    pipeline_ml.fit(X.iloc[tr], Y[tr])\n",
    "    oof_proba[te] = pipeline_ml.predict_proba(X.iloc[te])\n",
    "\n",
    "    te_texts = X.iloc[te]\n",
    "    discarded[te] = te_texts.apply(prefilter_to_none).values\n",
    "\n",
    "thresholds = []\n",
    "for j in range(len(POS_LABELS)):\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in np.linspace(0.05, 0.95, 19):\n",
    "        pred = (oof_proba[:, j] >= t).astype(int)\n",
    "        f1 = f1_score(Y[:, j], pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    thresholds.append(best_t)\n",
    "\n",
    "thr = np.array(thresholds)\n",
    "oof_model = (oof_proba >= thr).astype(int)\n",
    "\n",
    "oof_system = oof_model.copy()\n",
    "oof_system[discarded, :] = 0\n",
    "\n",
    "print(\"thresholds:\", dict(zip(POS_LABELS, thresholds)))\n",
    "\n",
    "print(\"\\n=== MULTILABEL: MODEL ONLY (thresholded) ===\")\n",
    "print(classification_report(Y, oof_model, target_names=POS_LABELS, digits=3, zero_division=0))\n",
    "\n",
    "print(\"\\n=== MULTILABEL: SYSTEM (PREFILTER + MODEL) ===\")\n",
    "print(classification_report(Y, oof_system, target_names=POS_LABELS, digits=3, zero_division=0))\n",
    "\n",
    "print(\"\\nDescartados por prefilter:\", int(discarded.sum()))\n",
    "print(df.loc[discarded, [\"type\", \"source\", \"content\", \"content_norm\"]].to_string(index=False))\n",
    "\n",
    "pipeline_ml.fit(df[\"content_norm\"], Y)\n",
    "\n",
    "def predict_labels(texts, use_prefilter=True):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    texts_norm = [normalize_placeholders(t) for t in texts]\n",
    "    proba = pipeline_ml.predict_proba(texts_norm)\n",
    "    pred = (proba >= thr).astype(int)\n",
    "\n",
    "    if use_prefilter:\n",
    "        disc = np.array([prefilter_to_none(tn) for tn in texts_norm], dtype=bool)\n",
    "        pred[disc, :] = 0\n",
    "\n",
    "    return texts_norm, proba, pred\n"
   ],
   "id": "47fb36f149482bc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: {'fake_urgency': 0.35, 'fake_scarcity': 0.7, 'shaming': 0.39999999999999997, 'social_proof': 0.3}\n",
      "\n",
      "=== MULTILABEL: MODEL ONLY (thresholded) ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " fake_urgency      0.770     0.906     0.833       192\n",
      "fake_scarcity      0.758     0.649     0.699        77\n",
      "      shaming      0.857     0.917     0.886        72\n",
      " social_proof      0.711     0.842     0.771       222\n",
      "\n",
      "    micro avg      0.755     0.847     0.798       563\n",
      "    macro avg      0.774     0.829     0.797       563\n",
      " weighted avg      0.756     0.847     0.797       563\n",
      "  samples avg      0.736     0.769     0.744       563\n",
      "\n",
      "\n",
      "=== MULTILABEL: SYSTEM (PREFILTER + MODEL) ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " fake_urgency      0.780     0.906     0.839       192\n",
      "fake_scarcity      0.758     0.649     0.699        77\n",
      "      shaming      0.868     0.917     0.892        72\n",
      " social_proof      0.719     0.842     0.776       222\n",
      "\n",
      "    micro avg      0.763     0.847     0.803       563\n",
      "    macro avg      0.781     0.829     0.801       563\n",
      " weighted avg      0.764     0.847     0.802       563\n",
      "  samples avg      0.736     0.769     0.744       563\n",
      "\n",
      "\n",
      "Descartados por prefilter: 6\n",
      "   type      source                                     content                                content_norm\n",
      "ninguno Desconocido                                  No gracias                                  No gracias\n",
      "ninguno          IA               Su sesión expira en 5 minutos         Su sesión expira en __NUM__ minutos\n",
      "ninguno          IA            El partido empieza en 15 minutos       El partido empieza en __NUM__ minutos\n",
      "ninguno          IA La batería del dispositivo se agotará en 2h La batería del dispositivo se agotará en 2h\n",
      "ninguno          IA           El examen comienza en 30 segundos      El examen comienza en __NUM__ segundos\n",
      "ninguno       boots                     Disponible próximamente                     Disponible próximamente\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T16:52:14.878748400Z",
     "start_time": "2026-01-10T16:52:14.859978900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Requiere:\n",
    "# df, Y, oof_system  (SYSTEM = prefilter + model)\n",
    "\n",
    "# =========================\n",
    "# Preparación\n",
    "# =========================\n",
    "A = df[[\"type\", \"source\", \"content\"]].copy()\n",
    "\n",
    "# Ground truth binario\n",
    "gt = (Y.sum(axis=1) > 0).astype(int)   # 1=pattern, 0=ninguno\n",
    "A[\"gt\"] = gt\n",
    "\n",
    "# Predicción binaria SYSTEM\n",
    "pred = (oof_system.sum(axis=1) > 0).astype(int)\n",
    "A[\"pred\"] = pred\n",
    "\n",
    "# =========================\n",
    "# Métricas helper\n",
    "# =========================\n",
    "def metrics(tp, fp, fn, tn):\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0\n",
    "    return round(prec,3), round(rec,3), round(f1,3)\n",
    "\n",
    "# =========================\n",
    "# Confusión binaria\n",
    "# =========================\n",
    "TP = int(((gt == 1) & (pred == 1)).sum())\n",
    "FP = int(((gt == 0) & (pred == 1)).sum())\n",
    "FN = int(((gt == 1) & (pred == 0)).sum())\n",
    "TN = int(((gt == 0) & (pred == 0)).sum())\n",
    "\n",
    "# =========================\n",
    "# Métricas\n",
    "# =========================\n",
    "p_pat, r_pat, f1_pat = metrics(TP, FP, FN, TN)\n",
    "p_non, r_non, f1_non = metrics(TN, FN, FP, TP)\n",
    "\n",
    "acc = (TP + TN) / len(A)\n",
    "\n",
    "print(\"===== METRICS (SYSTEM) =====\")\n",
    "print(f\"[PATTERN]  P={p_pat}  R={r_pat}  F1={f1_pat}\")\n",
    "print(f\"[NINGUNO]  P={p_non}  R={r_non}  F1={f1_non}\")\n",
    "print(f\"[GLOBAL ]  Accuracy={acc:.3f}\")\n",
    "\n",
    "print(\"\\n===== CONFUSION (pattern vs ninguno) =====\")\n",
    "print(pd.DataFrame(\n",
    "    [[TN, FP],\n",
    "     [FN, TP]],\n",
    "    index=[\"gt_ninguno\", \"gt_pattern\"],\n",
    "    columns=[\"pred_ninguno\", \"pred_pattern\"]\n",
    ").to_string())\n",
    "\n",
    "# =========================\n",
    "# Errores entre ambos\n",
    "# =========================\n",
    "print(\"\\n===== ERRORES CLAVE =====\")\n",
    "\n",
    "fps = A[(A[\"gt\"] == 0) & (A[\"pred\"] == 1)]\n",
    "fns = A[(A[\"gt\"] == 1) & (A[\"pred\"] == 0)]\n",
    "\n",
    "print(f\"\\nFALSE POSITIVES (ninguno → patrón): {len(fps)}\")\n",
    "for _, r in fps.head(15).iterrows():\n",
    "    print(f\"- {r['content']} | src={r['source']}\")\n",
    "\n",
    "print(f\"\\nFALSE NEGATIVES (patrón → ninguno): {len(fns)}\")\n",
    "for _, r in fns.head(15).iterrows():\n",
    "    print(f\"- {r['content']} | src={r['source']}\")\n"
   ],
   "id": "b4e2ce4f8a917012",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== METRICS (SYSTEM) =====\n",
      "[PATTERN]  P=0.912  R=0.945  F1=0.928\n",
      "[NINGUNO]  P=0.333  R=0.234  F1=0.275\n",
      "[GLOBAL ]  Accuracy=0.869\n",
      "\n",
      "===== CONFUSION (pattern vs ninguno) =====\n",
      "            pred_ninguno  pred_pattern\n",
      "gt_ninguno            15            49\n",
      "gt_pattern            30           511\n",
      "\n",
      "===== ERRORES CLAVE =====\n",
      "\n",
      "FALSE POSITIVES (ninguno → patrón): 49\n",
      "- Válido del 26/12 al 6/01 | src=fravega\n",
      "- ¡Tenés 30 días de entregas gratis! | src=rappi\n",
      "- 4 colores, 5 piezas | src=wish\n",
      "- 4 colores | src=wish\n",
      "- Hasta 27% de descuento | src=wish\n",
      "- 8 tamaños | src=wish\n",
      "- ARS2.100 de crédito por retraso | src=temu\n",
      "- Llega a AR en tan solo 3 días hábiles después del envío | src=temu\n",
      "- 558 ventas | src=temu\n",
      "- Quiero aprender SEO | src=aprendoseo\n",
      "- Quiero saber más | src=aprendoseo\n",
      "- Quiero más info | src=aprendoseo\n",
      "- Quiero mi código | src=LaNacion\n",
      "- Quiero una consultoría digital | src=JuanMenorio\n",
      "- No, lo quiero devolver | src=Amazon\n",
      "\n",
      "FALSE NEGATIVES (patrón → ninguno): 30\n",
      "- Continuar sin apoyarnos | src=Baeldung\n",
      "- Si, quiero más tráfico | src=NeilPatel\n",
      "- Fijá el precio más bajo | src=Desconocido\n",
      "- El precio sube hasta 469 USD en | src=Desconocido\n",
      "- Crystal Li en Flushing, Estados Unidos, realizó una compra | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- Alguien de Noruega compró un rallador Super FG - 12 veces más rápido | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- 39 lo compraron recientemente | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- Leah Rich en Louisville, Estados Unidos, realizó una compra | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- 404 Pedidos | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- Sabrina en Paraparaumu, Nueva Zelanda, compró chocolate energético SCHO-KA-KOLA | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- 3642 COMPRADOS | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- Jessica en Metairie, Estados Unidos, compró un vestido largo estilo playa | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- Ken en Three Bridges compró All Zebrawood + Olive Ash | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- 89 viendo actualmente | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n",
      "- 143 COMPRADOS | src=https://github.com/aruneshmathur/dark-patterns/blob/master/data/final-dark-patterns/dark-patterns.csv\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
